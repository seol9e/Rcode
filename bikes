# Glimpse at bike_share_rides
glimpse(bike_share_rides)
Rows: 35,229
Columns: 11
$ ride_id             <int> 52797, 54540, 87695, 45619, 70832, 96135, 29928, 8…
$ date                <chr> "2017-04-15", "2017-04-19", "2017-04-14", "2017-04…
$ duration            <chr> "1316.15 minutes", "8.13 minutes", "24.85 minutes"…
$ station_A_id        <dbl> 67, 21, 16, 58, 16, 6, 5, 16, 5, 81, 30, 16, 16, 6…
$ station_A_name      <chr> "San Francisco Caltrain Station 2  (Townsend St at…
$ station_B_id        <dbl> 89, 64, 355, 368, 81, 66, 350, 91, 62, 81, 109, 10…
$ station_B_name      <chr> "Division St at Potrero Ave", "5th St at Brannan S…
$ bike_id             <dbl> 1974, 860, 2263, 1417, 507, 75, 388, 239, 1449, 32…
$ user_gender         <chr> "Male", "Male", "Male", "Male", "Male", "Male", "M…
$ user_birth_year     <dbl> 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 19…
$ user_birth_year_fct <fct> 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 19…
# Summary of user_birth_year
summary(bike_share_rides$user_birth_year)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   1900    1979    1986    1984    1991    2001 
# Convert user_birth_year to factor: user_birth_year_fct
bike_share_rides <- bike_share_rides %>%
  mutate(user_birth_year_fct = as.factor(user_birth_year))
# Assert user_birth_year_fct is a factor
assert_is_factor(bike_share_rides$user_birth_year_fct)
# Summary of user_birth_year_fct
summary(bike_share_rides$user_birth_year_fct)
1900 1902 1923 1931 1938 1939 1941 1942 1943 1945 1946 1947 1948 1949 1950 1951 
   1    7    2   23    2    1    3   10    4   16    5   24    9   30   37   25 
1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 
  70   49   65   66  112   62  156   99  196  161  256  237  245  349  225  363 
1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 
 365  331  370  548  529  527  563  601  481  541  775  876  825 1016 1056 1262 
1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 
1157 1318 1606 1672 2135 1872 2062 1582 1703 1498 1476 1185  813  358  365  348 
2000 2001 
 473   30 
bike_share_rides <- bike_share_rides %>%
  # Remove 'minutes' from duration: duration_trimmed
  mutate(duration_trimmed = str_remove(duration, "minutes"),
         # Convert duration_trimmed to numeric: duration_mins
         duration_mins = as.numeric(duration_trimmed))
# Glimpse at bike_share_rides
glimpse(bike_share_rides)
Rows: 35,229
Columns: 12
$ ride_id          <int> 52797, 54540, 87695, 45619, 70832, 96135, 29928, 8333…
$ date             <chr> "2017-04-15", "2017-04-19", "2017-04-14", "2017-04-03…
$ duration         <chr> "1316.15 minutes", "8.13 minutes", "24.85 minutes", "…
$ station_A_id     <fct> 67, 21, 16, 58, 16, 6, 5, 16, 5, 81, 30, 16, 16, 67, …
$ station_A_name   <chr> "San Francisco Caltrain Station 2  (Townsend St at 4t…
$ station_B_id     <dbl> 89, 64, 355, 368, 81, 66, 350, 91, 62, 81, 109, 10, 8…
$ station_B_name   <chr> "Division St at Potrero Ave", "5th St at Brannan St",…
$ bike_id          <dbl> 1974, 860, 2263, 1417, 507, 75, 388, 239, 1449, 3289,…
$ user_gender      <chr> "Male", "Male", "Male", "Male", "Male", "Male", "Male…
$ user_birth_year  <dbl> 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 1993,…
$ duration_trimmed <chr> "1316.15 ", "8.13 ", "24.85 ", "6.35 ", "9.8 ", "17.4…
$ duration_mins    <dbl> 1316.15, 8.13, 24.85, 6.35, 9.80, 17.47, 16.52, 14.72…
# Assert duration_mins is numeric
assert_is_numeric(bike_share_rides$duration_mins)
# Calculate mean duration
mean(bike_share_rides$duration_mins)
[1] 13.06214
# Create breaks
breaks <- c(min(bike_share_rides$duration_min), 0, 1440, max(bike_share_rides$duration_min))

# Create a histogram of duration_min
ggplot(bike_share_rides, aes(duration_min)) +
  geom_histogram(breaks = breaks)

# duration_min_const: replace vals of duration_min > 1440 with 1440
bike_share_rides <- bike_share_rides %>%
  mutate(duration_min_const = replace(duration_min,duration_min> 1440, 1440))

# Make sure all values of duration_min_const are between 0 and 1440
assert_all_are_in_closed_range(bike_share_rides$duration_min_const, lower = 0, upper =1440)
library(lubridate)
# Convert date to Date type
bike_share_rides <- bike_share_rides %>%
  mutate(date = as.Date(bike_share_rides$date))

# Make sure all dates are in the past
assert_all_are_in_past(bike_share_rides$date)

# Filter for rides that occurred before or on today's date
bike_share_rides_past <- bike_share_rides %>%
  filter(date<=today())

# Make sure all dates from bike_share_rides_past are in the past
assert_all_are_in_past(bike_share_rides_past$date)
# Count the number of full duplicates
sum(duplicated(bike_share_rides))

# Remove duplicates
bike_share_rides_unique <- unique(bike_share_rides)

# Count the full duplicates in bike_share_rides_unique
sum(duplicated(bike_share_rides_unique))
# Find duplicated ride_ids
bike_share_rides %>% 
  # Count the number of occurrences of each ride_id
  count(ride_id) %>% 
  # Filter for rows with a count > 1
  filter(n>1)
# A tibble: 2 × 2
  ride_id     n
    <int> <int>
1   41441     2
2   87056     2
# Find duplicated ride_ids
bike_share_rides %>% 
  count(ride_id) %>% 
  filter(n > 1)

# Remove full and partial duplicates
bike_share_rides_unique <- bike_share_rides %>%
  # Only based on ride_id instead of all cols
  distinct(ride_id, .keep_all = TRUE)

# Find duplicated ride_ids in bike_share_rides_unique
bike_share_rides_unique %>%
  # Count the number of occurrences of each ride_id
  count(ride_id) %>%
  # Filter for rows with a count > 1
  filter(n>1)

bike_share_rides %>%
  # Group by ride_id and date
  group_by(ride_id,date) %>%
  # Add duration_min_avg column
  mutate(duration_min_avg = mean(duration_min) ) %>%
  # Remove duplicates based on ride_id and date, keep all cols
  distinct(ride_id,date,.keep_all=TRUE) %>%
  # Remove duration_min column
  select(-duration_min)
# A tibble: 35,229 × 10
# Groups:   ride_id, date [35,229]
   ride_id date       station_…¹ stati…² stati…³ stati…⁴ bike_id user_…⁵ user_…⁶
     <int> <date>     <fct>      <chr>   <fct>   <chr>   <fct>   <fct>     <dbl>
 1   52797 2017-04-15 67         San Fr… 89      Divisi… 1974    Male       1972
 2   54540 2017-04-19 21         Montgo… 64      5th St… 860     Male       1986
 3   87695 2017-04-14 16         Steuar… 355     23rd S… 2263    Male       1993
 4   45619 2017-04-03 58         Market… 368     Myrtle… 1417    Male       1981
 5   70832 2017-04-10 16         Steuar… 81      Berry … 507     Male       1981
 6   96135 2017-04-18 6          The Em… 66      3rd St… 75      Male       1988
 7   29928 2017-04-22 5          Powell… 350     8th St… 388     Male       1993
 8   83331 2017-04-11 16         Steuar… 91      Berry … 239     Male       1996
 9   72424 2017-04-05 5          Powell… 62      Victor… 1449    Male       1993
10   25910 2017-04-20 81         Berry … 81      Berry … 3289    Male       1996
# … with 35,219 more rows, 1 more variable: duration_min_avg <dbl>, and
#   abbreviated variable names ¹​station_A_id, ²​station_A_name, ³​station_B_id,
#   ⁴​station_B_name, ⁵​user_gender, ⁶​user_birth_year


>
glimpse(sfo_survey)
# Find bad dest_size rows
sfo_survey %>% 
  # Join with dest_sizes data frame to get bad dest_size rows
  anti_join(dest_sizes, by="dest_size") %>%
  # Select id, airline, destination, and dest_size cols
  select(id,airline,destination,dest_size)
    id     airline       destination dest_size
1  278      UNITED         BALTIMORE      huge
2  982   LUFTHANSA            MUNICH       Hub
3 2063    AMERICAN      PHILADELPHIA   Large  
4  777 UNITED INTL SAN JOSE DEL CABO   Small  

# Remove bad dest_size rows
sfo_survey %>% 
  # Join with dest_sizes
  semi_join(dest_sizes,by="dest_size")%>%
  # Count the number of each dest_size
  count(dest_size)
  dest_size    n
1       Hub 1756
2     Large  143
3    Medium  681
4     Small  225
# Count dest_size
sfo_survey %>%
  count(dest_size)
  dest_size    n
1   Small      1
2       Hub    1
3       Hub 1756
4     Large  143
5   Large      1
6    Medium  682
7     Small  225
# Count cleanliness
sfo_survey %>%
  count(cleanliness)
     cleanliness    n
1        average    1
2        Average  431
3        AVERAGE    1
4          Clean  970
5          Dirty    2
6 somewhat clean    1
7 Somewhat clean 1253
8 Somewhat dirty   30
9           <NA>  120

# Add new columns to sfo_survey
sfo_survey <- sfo_survey %>%
  # dest_size_trimmed: dest_size without whitespace
  mutate(dest_size_trimmed = str_trim(dest_size),
         # cleanliness_lower: cleanliness converted to lowercase
         cleanliness_lower = str_to_lower(cleanliness))

# Count values of dest_size_trimmed
sfo_survey %>%
  count(dest_size_trimmed)

# Count values of cleanliness_lower
sfo_survey %>%
  count(cleanliness_lower)
# Count categories of dest_region
sfo_survey %>%
  count(dest_region)
             dest_region   n
1                   Asia 260
2  Australia/New Zealand  66
3          Canada/Mexico 220
4  Central/South America  29
5                East US 498
6                     EU   2
7                    eur   2
8                  Europ   1
9                 Europe 396
10           Middle East  79
11            Midwest US 281
12               West US 975
# Count categories of dest_region
sfo_survey %>%
  count(dest_region)

# Categories to map to Europe
europe_categories <- c("EU", "eur", "Europ")

# Add a new col dest_region_collapsed
sfo_survey %>%
  # Map all categories in europe_categories to Europe
  mutate(dest_region_collapsed = fct_collapse(dest_region, 
                                     Europe = europe_categories)) %>%
  # Count categories of dest_region_collapsed
  count(dest_region_collapsed)

# Filter for rows with "(" or ")" in the phone column
sfo_survey %>%
  filter(str_detect(phone, fixed("(")) | str_detect(phone, fixed(")")))
# Remove parentheses from phone column
phone_no_parens <- sfo_survey$phone %>%
  # Remove "("s
  str_remove_all(fixed("(")) %>%
  # Remove ")"s
  str_remove_all(fixed(")"))

# Add phone_no_parens as column
sfo_survey %>%
  mutate(phone_no_parens = phone_no_parens,
  # Replace all hyphens in phone_no_parens with spaces
         phone_clean =str_replace_all(phone_no_parens,"-"," ") )
# Check out the invalid numbers
sfo_survey %>%
  filter(str_length(phone)==12)

# Remove rows with invalid numbers
sfo_survey %>%
  filter(str_length(phone)!=12)
# Check out the accounts data frame
head(accounts)

# Define the date formats
formats <- c("%Y-%m-%d", "%B %d, %Y")

# Convert dates to the same format
accounts %>%
  mutate(date_opened_clean = parse_date_time(date_opened,formats))
# Scatter plot of opening date and total amount
accounts %>%
  ggplot(aes(x = date_opened, y = total)) +
  geom_point()

# Left join accounts to account_offices by id
accounts %>%
  left_join(account_offices, by = "id") %>%
  # Convert totals from the Tokyo office to USD
  mutate(total_usd = ifelse(office == "Tokyo", total / 104, total)) %>%
  # Scatter plot of opening date vs total_usd
  ggplot(aes(x = date_opened, y = total_usd)) +
    geom_point()
# Find invalid totals
accounts %>%
  # theoretical_total: sum of the three funds
  mutate(theoretical_total=fund_A+fund_B+fund_C) %>%
  # Find accounts where total doesn't match theoretical_total
  filter(total!=theoretical_total)
# Find invalid acct_age
accounts %>%
  # theoretical_age: age of acct based on date_opened
  mutate(theoretical_age = floor(as.numeric(date_opened%--%today(),"years"))) %>%
  # Filter for rows where acct_age is different from theoretical_age
  filter(acct_age!=theoretical_age)
# Visualize the missing values by column
vis_miss(accounts)
accounts %>%
  # missing_inv: Is inv_amount missing?
  mutate(missing_inv = is.na(inv_amount)) %>%
  # Group by missing_inv
  group_by(missing_inv)%>%
  # Calculate mean age for each missing_inv group
  summarize(avg_age = mean(age))
# A tibble: 2 × 2
  missing_inv avg_age
  <lgl>         <dbl>
1 FALSE          43.6
2 TRUE           21.8
accounts %>%
  # missing_inv: Is inv_amount missing?
  mutate(missing_inv = is.na(inv_amount)) %>%
  # Group by missing_inv
  group_by(missing_inv) %>%
  # Calculate mean age for each missing_inv group
  summarize(avg_age = mean(age))

# Sort by age and visualize missing vals
accounts %>%
  arrange(age) %>%
  vis_miss()
# Create accounts_clean
accounts_clean <- accounts %>%
  # Filter to remove rows with missing cust_id
  filter(!is.na(cust_id)) %>%
  # Add new col acct_amount_filled with replaced NAs
  mutate(acct_amount_filled = ifelse(is.na(acct_amount), inv_amount * 5, acct_amount))

# Assert that cust_id has no missing vals
assert_all_are_not_na(accounts_clean$cust_id)

# Assert that acct_amount_filled has no missing vals
assert_all_are_not_na(accounts_clean$acct_amount_filled)
# Count the number of each city variation
zagat %>%
  count(city)

# Join zagat and cities and look at results
zagat %>%
  # Left join based on stringdist using city and city_actual cols
  stringdist_left_join(cities, by = c("city" = "city_actual")) %>%
  # Select the name, city, and city_actual cols
  select(name,city, city_actual)
# Load reclin
library(reclin)

# Generate pairs with same city
pair_blocking(zagat, fodors, blocking_var="city")
